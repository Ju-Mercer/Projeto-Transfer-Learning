{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg+lw5KSo/YUvcLhiBzbr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ju-Mercer/Projeto-Transfer-Learning/blob/main/Projeto_Transfer_Learning_cat_vs_dog_Juli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3g8B5FI-K-E2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Objetivo é criar projeto usando transfer learning para treinar uma rede neural a distinguir uma gramímea especifíca pouco estudada: Braquiaria plantaginea de outras pertencentes ao gënero. Para isso utilizarei um banco de dado que criei com 100 imagens da espécie de interesse e outro, com 100 fotos de outras especies de gramíneas. S"
      ],
      "metadata": {
        "id": "AvEafTy9ORzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "estou treinando a aula do prof Dieto da DIO.ME da aula Ïmplementando uma deep learning do zero no Python\""
      ],
      "metadata": {
        "id": "M4AwkG9cUC7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversao de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #Carrega a parte de treino do Dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #Carrega a parte de validacao\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "3mEUAcp9XSzL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = next(dataiter)\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "a8_NRRQEbk9L",
        "outputId": "60494cc9-1db2-4ae1-cfe9-db4a20db1cdf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG0JJREFUeJzt3X9sVfX9x/HX5UevgO3FWtrbOworP4RNoHMoXaPyRWko3UYA2YI/loFTiKy4AXOabiq6H3ZCIkbDYMkm6BRUNoFIHBOLLXEWDCghzK2hTScl0DJJuLcUWwj9fP8g3nGlCOdyb9/t5flITkLvPZ/et8dLn5ze21Ofc84JAIAu1st6AADAlYkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE32sB/iijo4OHT58WOnp6fL5fNbjAAA8cs6ppaVFoVBIvXpd+Dyn2wXo8OHDysvLsx4DAHCZGhsbNXjw4Ave3+0ClJ6eLuns4BkZGcbTAAC8ikQiysvLi349v5CkBWjlypVavny5mpqaVFBQoOeff14TJky46LrPv+2WkZFBgACgB7vYyyhJeRPCa6+9piVLlmjp0qX68MMPVVBQoJKSEh09ejQZDwcA6IGSEqBnnnlG8+bN07333quvf/3rWr16tfr3768XXnghGQ8HAOiBEh6gU6dOac+ePSouLv7fg/TqpeLiYtXU1Jy3f3t7uyKRSMwGAEh9CQ/Qp59+qjNnzignJyfm9pycHDU1NZ23f0VFhQKBQHTjHXAAcGUw/0HU8vJyhcPh6NbY2Gg9EgCgCyT8XXBZWVnq3bu3mpubY25vbm5WMBg8b3+/3y+/35/oMQAA3VzCz4DS0tI0fvx4VVZWRm/r6OhQZWWlioqKEv1wAIAeKik/B7RkyRLNmTNHN954oyZMmKBnn31Wra2tuvfee5PxcACAHigpAZo9e7b++9//6vHHH1dTU5O+8Y1vaOvWree9MQEAcOXyOeec9RDnikQiCgQCCofDXAkBAHqgS/06bv4uOADAlYkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw0cd6AOBiHnvsMc9rXnzxxbge63vf+57nNStWrPC8Zvr06Z7XFBYWel5TXl7ueU0q+uCDD+Ja953vfMfzmrffftvzmhtuuMHzmlTAGRAAwAQBAgCYSHiAnnjiCfl8vpht9OjRiX4YAEAPl5TXgK6//nq98847/3uQPrzUBACIlZQy9OnTR8FgMBmfGgCQIpLyGtCBAwcUCoU0bNgw3XPPPTp48OAF921vb1ckEonZAACpL+EBKiws1Nq1a7V161atWrVKDQ0NuvXWW9XS0tLp/hUVFQoEAtEtLy8v0SMBALqhhAeotLRU3//+9zVu3DiVlJTorbfe0vHjx/X66693un95ebnC4XB0a2xsTPRIAIBuKOnvDhg4cKCuu+461dXVdXq/3++X3+9P9hgAgG4m6T8HdOLECdXX1ys3NzfZDwUA6EESHqCHHnpI1dXV+s9//qP3339fM2fOVO/evXXXXXcl+qEAAD1Ywr8Fd+jQId111106duyYBg0apFtuuUU7d+7UoEGDEv1QAIAezOecc9ZDnCsSiSgQCCgcDisjI8N6HCTYyy+/7HlNPBcj/eSTTzyvSUXf/e5341r3l7/8xfOatLS0uB6rK5w5cyaudW1tbZ7XDBgwIK7HSiWX+nWca8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACaS/gvpgHP99re/9bymKy8sGggEPK/54Q9/6HnNI4884nlNV+rTJ7W+NPTu3TuudVxYNLk4AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJ1LrkLXCZli9f7nnN/fffn4RJgNTHGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQLnCIfD1iMAVwzOgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE1yMFF3qoYce8rxm8eLFntecOHHC8xpJevrppz2vuf322z2vueGGGzyvAVINZ0AAABMECABgwnOAduzYoWnTpikUCsnn82nTpk0x9zvn9Pjjjys3N1f9+vVTcXGxDhw4kKh5AQApwnOAWltbVVBQoJUrV3Z6/7Jly/Tcc89p9erV2rVrlwYMGKCSkhK1tbVd9rAAgNTh+U0IpaWlKi0t7fQ+55yeffZZPfroo5o+fbok6aWXXlJOTo42bdqkO++88/KmBQCkjIS+BtTQ0KCmpiYVFxdHbwsEAiosLFRNTU2na9rb2xWJRGI2AEDqS2iAmpqaJEk5OTkxt+fk5ETv+6KKigoFAoHolpeXl8iRAADdlPm74MrLyxUOh6NbY2Oj9UgAgC6Q0AAFg0FJUnNzc8ztzc3N0fu+yO/3KyMjI2YDAKS+hAYoPz9fwWBQlZWV0dsikYh27dqloqKiRD4UAKCH8/wuuBMnTqiuri76cUNDg/bu3avMzEwNGTJEixYt0m9+8xuNHDlS+fn5euyxxxQKhTRjxoxEzg0A6OE8B2j37t267bbboh8vWbJEkjRnzhytXbtWDz/8sFpbWzV//nwdP35ct9xyi7Zu3aqrrroqcVMDAHo8n3POWQ9xrkgkokAgoHA4zOtBkCTNnj3b85q33norrsfq1cv7d6VXrVrleU13vxjpqFGjPK+J59ghNV3q13GeMQAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDB1bCRkl544YW41i1cuNDzmra2trgeqyvE+9f7rrvu8rxm0qRJntfMnz/f8xp0f1wNGwDQrREgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJvpYDwAkw49+9KO41uXm5npeU19f73nNli1bPK95++23Pa+J16uvvup5jd/v97yGi5Fe2TgDAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM+JxzznqIc0UiEQUCAYXDYWVkZFiPAyRFe3u75zWlpaWe17z77rue10iSz+fzvGbs2LGe17zzzjue1wwaNMjzGnStS/06zhkQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCi5ECKeypp56Ka90vf/nLBE/Sueuvv97zmm3btnlek5ub63kN4sfFSAEA3RoBAgCY8BygHTt2aNq0aQqFQvL5fNq0aVPM/XPnzpXP54vZpk6dmqh5AQApwnOAWltbVVBQoJUrV15wn6lTp+rIkSPRbf369Zc1JAAg9fTxuqC0tPSiv5nR7/crGAzGPRQAIPUl5TWgqqoqZWdna9SoUVqwYIGOHTt2wX3b29sViURiNgBA6kt4gKZOnaqXXnpJlZWVevrpp1VdXa3S0lKdOXOm0/0rKioUCASiW15eXqJHAgB0Q56/BXcxd955Z/TPY8eO1bhx4zR8+HBVVVVp8uTJ5+1fXl6uJUuWRD+ORCJECACuAEl/G/awYcOUlZWlurq6Tu/3+/3KyMiI2QAAqS/pATp06JCOHTvGTyIDAGJ4/hbciRMnYs5mGhoatHfvXmVmZiozM1NPPvmkZs2apWAwqPr6ej388MMaMWKESkpKEjo4AKBn8xyg3bt367bbbot+/PnrN3PmzNGqVau0b98+vfjiizp+/LhCoZCmTJmiX//61/L7/YmbGgDQ43kO0KRJk/Rl1y/9+9//flkDAUicG2+8Ma518fyD8dSpU57XfPzxx57XHDp0yPMaXgLonrgWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwk/FdyA+g+amtr41p3+vTpBE8CnI8zIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABBcjBQzEc7HPn/zkJ57X/PWvf/W8RpKcc3Gt8yoUCnlec8011yRhEljgDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSIHL1NbW5nnNtm3bPK/5wx/+4HlNvBcV9fl8ntfEc2HR+++/3/OaESNGeF6D7okzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABBcjBc7x2WefeV5TWVnpec306dM9r+nu7r33Xs9rnnjiicQPgh6DMyAAgAkCBAAw4SlAFRUVuummm5Senq7s7GzNmDFDtbW1Mfu0tbWprKxM1157ra6++mrNmjVLzc3NCR0aANDzeQpQdXW1ysrKtHPnTm3btk2nT5/WlClT1NraGt1n8eLFevPNN7VhwwZVV1fr8OHDuuOOOxI+OACgZ/P0JoStW7fGfLx27VplZ2drz549mjhxosLhsP70pz9p3bp1uv322yVJa9as0de+9jXt3LlT3/rWtxI3OQCgR7us14DC4bAkKTMzU5K0Z88enT59WsXFxdF9Ro8erSFDhqimpqbTz9He3q5IJBKzAQBSX9wB6ujo0KJFi3TzzTdrzJgxkqSmpialpaVp4MCBMfvm5OSoqamp089TUVGhQCAQ3fLy8uIdCQDQg8QdoLKyMu3fv1+vvvrqZQ1QXl6ucDgc3RobGy/r8wEAeoa4fhB14cKF2rJli3bs2KHBgwdHbw8Ggzp16pSOHz8ecxbU3NysYDDY6efy+/3y+/3xjAEA6ME8nQE557Rw4UJt3LhR27dvV35+fsz948ePV9++fWN+Mry2tlYHDx5UUVFRYiYGAKQET2dAZWVlWrdunTZv3qz09PTo6zqBQED9+vVTIBDQfffdpyVLligzM1MZGRl68MEHVVRUxDvgAAAxPAVo1apVkqRJkybF3L5mzRrNnTtXkrRixQr16tVLs2bNUnt7u0pKSvT73/8+IcMCAFKHzznnrIc4VyQSUSAQUDgcVkZGhvU4uMI8+uijntc89dRTSZgkMeL9671+/XrPa2bMmOF5zVVXXeV5Dbq/S/06zrXgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCKu34gKdKU///nPnteMHDkyrsd6+eWXPa+55pprPK8ZMGCA5zXxmDhxYlzrZs2a5XlN375943osXLk4AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAxUsRt165dntccPXrU85qHH37Y85rm5mbPayRp6NChnte8//77nteMGjXK8xog1XAGBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GKkiFv//v09r/nnP//pec20adM8r/njH//oeY0krVixwvMaLiwKxIczIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhM8556yHOFckElEgEFA4HFZGRob1OAAAjy716zhnQAAAEwQIAGDCU4AqKip00003KT09XdnZ2ZoxY4Zqa2tj9pk0aZJ8Pl/M9sADDyR0aABAz+cpQNXV1SorK9POnTu1bds2nT59WlOmTFFra2vMfvPmzdORI0ei27JlyxI6NACg5/P0G1G3bt0a8/HatWuVnZ2tPXv2aOLEidHb+/fvr2AwmJgJAQAp6bJeAwqHw5KkzMzMmNtfeeUVZWVlacyYMSovL9fJkycv+Dna29sViURiNgBA6vN0BnSujo4OLVq0SDfffLPGjBkTvf3uu+/W0KFDFQqFtG/fPj3yyCOqra3VG2+80ennqaio0JNPPhnvGACAHirunwNasGCB/va3v+m9997T4MGDL7jf9u3bNXnyZNXV1Wn48OHn3d/e3q729vbox5FIRHl5efwcEAD0UJf6c0BxnQEtXLhQW7Zs0Y4dO740PpJUWFgoSRcMkN/vl9/vj2cMAEAP5ilAzjk9+OCD2rhxo6qqqpSfn3/RNXv37pUk5ebmxjUgACA1eQpQWVmZ1q1bp82bNys9PV1NTU2SpEAgoH79+qm+vl7r1q3Tt7/9bV177bXat2+fFi9erIkTJ2rcuHFJ+Q8AAPRMnl4D8vl8nd6+Zs0azZ07V42NjfrBD36g/fv3q7W1VXl5eZo5c6YeffTRS349h2vBAUDPlpTXgC7Wqry8PFVXV3v5lACAKxTXggMAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOhjPcAXOeckSZFIxHgSAEA8Pv/6/fnX8wvpdgFqaWmRJOXl5RlPAgC4HC0tLQoEAhe83+culqgu1tHRocOHDys9PV0+ny/mvkgkory8PDU2NiojI8NoQnsch7M4DmdxHM7iOJzVHY6Dc04tLS0KhULq1evCr/R0uzOgXr16afDgwV+6T0ZGxhX9BPscx+EsjsNZHIezOA5nWR+HLzvz+RxvQgAAmCBAAAATPSpAfr9fS5culd/vtx7FFMfhLI7DWRyHszgOZ/Wk49Dt3oQAALgy9KgzIABA6iBAAAATBAgAYIIAAQBM9JgArVy5Ul/96ld11VVXqbCwUB988IH1SF3uiSeekM/ni9lGjx5tPVbS7dixQ9OmTVMoFJLP59OmTZti7nfO6fHHH1dubq769eun4uJiHThwwGbYJLrYcZg7d+55z4+pU6faDJskFRUVuummm5Senq7s7GzNmDFDtbW1Mfu0tbWprKxM1157ra6++mrNmjVLzc3NRhMnx6Uch0mTJp33fHjggQeMJu5cjwjQa6+9piVLlmjp0qX68MMPVVBQoJKSEh09etR6tC53/fXX68iRI9Htvffesx4p6VpbW1VQUKCVK1d2ev+yZcv03HPPafXq1dq1a5cGDBigkpIStbW1dfGkyXWx4yBJU6dOjXl+rF+/vgsnTL7q6mqVlZVp586d2rZtm06fPq0pU6aotbU1us/ixYv15ptvasOGDaqurtbhw4d1xx13GE6deJdyHCRp3rx5Mc+HZcuWGU18Aa4HmDBhgisrK4t+fObMGRcKhVxFRYXhVF1v6dKlrqCgwHoMU5Lcxo0box93dHS4YDDoli9fHr3t+PHjzu/3u/Xr1xtM2DW+eBycc27OnDlu+vTpJvNYOXr0qJPkqqurnXNn/9/37dvXbdiwIbrPv/71LyfJ1dTUWI2ZdF88Ds4593//93/upz/9qd1Ql6DbnwGdOnVKe/bsUXFxcfS2Xr16qbi4WDU1NYaT2Thw4IBCoZCGDRume+65RwcPHrQeyVRDQ4Oamppinh+BQECFhYVX5POjqqpK2dnZGjVqlBYsWKBjx45Zj5RU4XBYkpSZmSlJ2rNnj06fPh3zfBg9erSGDBmS0s+HLx6Hz73yyivKysrSmDFjVF5erpMnT1qMd0Hd7mKkX/Tpp5/qzJkzysnJibk9JydH//73v42mslFYWKi1a9dq1KhROnLkiJ588kndeuut2r9/v9LT063HM9HU1CRJnT4/Pr/vSjF16lTdcccdys/PV319vX7xi1+otLRUNTU16t27t/V4CdfR0aFFixbp5ptv1pgxYySdfT6kpaVp4MCBMfum8vOhs+MgSXfffbeGDh2qUCikffv26ZFHHlFtba3eeOMNw2ljdfsA4X9KS0ujfx43bpwKCws1dOhQvf7667rvvvsMJ0N3cOedd0b/PHbsWI0bN07Dhw9XVVWVJk+ebDhZcpSVlWn//v1XxOugX+ZCx2H+/PnRP48dO1a5ubmaPHmy6uvrNXz48K4es1Pd/ltwWVlZ6t2793nvYmlublYwGDSaqnsYOHCgrrvuOtXV1VmPYubz5wDPj/MNGzZMWVlZKfn8WLhwobZs2aJ333035te3BINBnTp1SsePH4/ZP1WfDxc6Dp0pLCyUpG71fOj2AUpLS9P48eNVWVkZva2jo0OVlZUqKioynMzeiRMnVF9fr9zcXOtRzOTn5ysYDMY8PyKRiHbt2nXFPz8OHTqkY8eOpdTzwzmnhQsXauPGjdq+fbvy8/Nj7h8/frz69u0b83yora3VwYMHU+r5cLHj0Jm9e/dKUvd6Pli/C+JSvPrqq87v97u1a9e6jz/+2M2fP98NHDjQNTU1WY/WpX72s5+5qqoq19DQ4P7xj3+44uJil5WV5Y4ePWo9WlK1tLS4jz76yH300UdOknvmmWfcRx995D755BPnnHO/+93v3MCBA93mzZvdvn373PTp011+fr777LPPjCdPrC87Di0tLe6hhx5yNTU1rqGhwb3zzjvum9/8phs5cqRra2uzHj1hFixY4AKBgKuqqnJHjhyJbidPnozu88ADD7ghQ4a47du3u927d7uioiJXVFRkOHXiXew41NXVuV/96ldu9+7drqGhwW3evNkNGzbMTZw40XjyWD0iQM459/zzz7shQ4a4tLQ0N2HCBLdz507rkbrc7NmzXW5urktLS3Nf+cpX3OzZs11dXZ31WEn37rvvOknnbXPmzHHOnX0r9mOPPeZycnKc3+93kydPdrW1tbZDJ8GXHYeTJ0+6KVOmuEGDBrm+ffu6oUOHunnz5qXcP9I6+++X5NasWRPd57PPPnM//vGP3TXXXOP69+/vZs6c6Y4cOWI3dBJc7DgcPHjQTZw40WVmZjq/3+9GjBjhfv7zn7twOGw7+Bfw6xgAACa6/WtAAIDURIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY+H/sSudhVTa22AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensoes do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #para verificar a dimensao davetiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUz2_CYHeNms",
        "outputId": "81a1bbc9-c3a2-4020-b26d-26f84746a533"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) #camada de entrada, 784 neuronios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) #camada interna1, 128 neuronios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) #camada interna2, 64 neuronios que se ligam a 10\n",
        "    # para a camada de saída náo é necessario definir nada pois só preisamos pegar o output da camada interna2\n",
        "\n",
        "  def  forwart(self, x):\n",
        "    X = F.rule(self.linear1(X)) # funcáo de ativacao da camada de entrada1\n",
        "    X = F.relu(self.linear2(X)) # funcão de ativacao da camada interna1 para a camada interna2\n",
        "    X = self.linear3(X) # funcão de ativacao da camada interna2 para a camada de saída, nesse cado f(x) = x\n",
        "    return F.log_softmax(X, dim=1) #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "DF41-UDkjQDu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #definindo o otimizador, ou definindo a politica de atualizacao de pesos e de bias\n",
        "  inicio = time() #timer para contar quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() #definindo a funcao de perda\n",
        "  EPOCHS = 10 #numero que o algoritmo rodará, iniciar teste com 10 e depois aumentar para 30\n",
        "  modelo.train() #ativando o modo de treinamento de modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 #inicia a perda acumulada em 0\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) #convertendo as imagens para vetores de 28*28 para fi\n",
        "      otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterios\n",
        "\n",
        "      output = modelo(imagens.to(device)) #colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) #calculando a perda do epoch em questao\n",
        "\n",
        "      perda_instantanea.backward() #back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() #atualizando os pesos e os bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() #acumulando a perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "      print(\"\\nTempo de treino (em minutos) =\", (time() - inicio)/60)\n"
      ],
      "metadata": {
        "id": "cYQJIwFWmk7l"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import log\n",
        "def validacao(modelo,valloader, device):\n",
        "    conta_corretas, conta_todas = 0, 0\n",
        "    for imagens, etiquetas in valloader:\n",
        "        for i in range(len(etiquetas)):\n",
        "            img = imagens[i].view(1, 784)\n",
        "            # desativa a diferencacao automatica para acelerar a validacao. graficos computacionais dinamicos\n",
        "            with torch.no_grad():\n",
        "              logps = modelo(img.to(device)) #output do modelo em escala logaritmica\n",
        "\n",
        "            ps = torch.exp(logps) #converte output para escala normal, lembrando que é um tensor\n",
        "            probab = list(ps.cpu().numpy()[0])\n",
        "            etiqueta_pred = probab.index(max(probab)) #converte o tensor em um numero, no caso o numero que o modelo previu\n",
        "            etiqueta_certa = etiquetas.numpy()[i]\n",
        "            if(etiqueta_certa == etiqueta_pred): #compara a previsao com o valor correto\n",
        "                conta_corretas += 1\n",
        "            conta_todas += 1\n",
        "\n",
        "    print(\"Total de imagens testadas =\", conta_todas)\n",
        "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))\n",
        "\n"
      ],
      "metadata": {
        "id": "8mLLXkB1r_Fh"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZzNsLXfvTTU",
        "outputId": "b8588f10-c7f0-410f-a079-b8b9ddb25c50"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oz8a0PN5vfJj"
      }
    }
  ]
}